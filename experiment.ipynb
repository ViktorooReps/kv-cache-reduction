{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-09T15:42:55.606689Z",
     "start_time": "2025-04-09T15:42:52.950704Z"
    }
   },
   "source": [
    "from model.modeling_llama import LlamaForCausalLM as ModifiedLlama\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from kvcache.iterative import IterativeReduceKVBiasCache as ModifiedCache\n",
    "from transformers import DynamicCache\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from IPython.display import DisplayHandle\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "DEVICE = 'mps'\n",
    "DTYPE = torch.float32\n",
    "FIRST_N = 1000\n",
    "SAMPLE_SIZE = 10"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:42:55.613839Z",
     "start_time": "2025-04-09T15:42:55.607649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the cache file name\n",
    "CACHE_FILENAME = f\"fineweb_sample{SAMPLE_SIZE}of{FIRST_N}.csv\"\n",
    "\n",
    "# Check if the cache file already exists\n",
    "if os.path.exists(CACHE_FILENAME):\n",
    "    print(f\"Cache file already exists: {CACHE_FILENAME}\")\n",
    "    df = pd.read_csv(CACHE_FILENAME)\n",
    "else:\n",
    "    # Load streaming dataset\n",
    "    dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", split=\"train\", name=\"sample-10BT\", streaming=True)\n",
    "    stream = iter(dataset)\n",
    "\n",
    "    # Take 1000 streamed samples\n",
    "    samples = [next(stream) for _ in range(1000)]\n",
    "\n",
    "    # Randomly select 10 of them\n",
    "    selected_samples = random.sample(samples, 10)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(selected_samples)\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(CACHE_FILENAME, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved CSV with {len(df)} samples to: {CACHE_FILENAME}\")\n",
    "\n",
    "texts = df[\"text\"]"
   ],
   "id": "388ca5340cb569b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache file already exists: fineweb_sample10of1000.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:42:55.618411Z",
     "start_time": "2025-04-09T15:42:55.614411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stepwise_perplexity(model, tokenizer, texts, cache_impl, update_every=10):\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    global_start = time.time()\n",
    "    total_texts = len(texts)\n",
    "\n",
    "    display_handle = DisplayHandle()\n",
    "    display_handle.display(\"Starting perplexity evaluation...\")\n",
    "\n",
    "    for text_idx, text in enumerate(texts):\n",
    "        enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "        input_ids = enc[\"input_ids\"].squeeze(0).to(model.device)\n",
    "        cache = cache_impl()\n",
    "        seq_len = input_ids.size(0)\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            input_slice = input_ids[i - 1 : i].unsqueeze(0)  # [1, 1]\n",
    "            label_token = input_ids[i].unsqueeze(0)          # [1]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(\n",
    "                    input_ids=input_slice,\n",
    "                    use_cache=True,\n",
    "                    past_key_values=cache,\n",
    "                )\n",
    "                cache = output.past_key_values\n",
    "                logits = output.logits[:, -1, :]  # [1, vocab_size]\n",
    "                loss = loss_fn(logits, label_token)\n",
    "                total_loss += loss.item()\n",
    "                total_tokens += 1\n",
    "\n",
    "            # Only update output every N steps for smooth UX\n",
    "            if total_tokens % update_every == 0 or (i == seq_len - 1 and text_idx == total_texts - 1):\n",
    "                elapsed = time.time() - global_start\n",
    "                avg_step_time = elapsed / max(total_tokens, 1)\n",
    "                remaining_steps = sum(len(tokenizer(t, truncation=True, max_length=256)[\"input_ids\"]) - 1 for t in texts) - total_tokens\n",
    "                eta_minutes = (avg_step_time * remaining_steps) / 60\n",
    "                current_ppl = np.exp(total_loss / total_tokens)\n",
    "\n",
    "                status = (f\"Text {text_idx + 1}/{total_texts} | \"\n",
    "                          f\"Token {i + 1}/{seq_len} \"\n",
    "                          f\"Global Steps: {total_tokens} | \"\n",
    "                          f\"ETA: {eta_minutes:.1f} min \"\n",
    "                          f\"Cumulative PPL: {current_ppl:.2f}\")\n",
    "                display_handle.update(status)\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    return np.exp(total_loss / total_tokens)\n"
   ],
   "id": "8b865fd89b4ffa7e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:42:55.620499Z",
     "start_time": "2025-04-09T15:42:55.619009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model_hf = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "# model_hf.eval().to(DEVICE).to(DTYPE)\n",
    "# \n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")"
   ],
   "id": "919834ab237639a4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:42:55.622989Z",
     "start_time": "2025-04-09T15:42:55.621695Z"
    }
   },
   "cell_type": "code",
   "source": "# stepwise_perplexity(model_hf, tokenizer, texts, cache_impl=lambda: None)",
   "id": "d59adc284110d7c2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:42:57.135234Z",
     "start_time": "2025-04-09T15:42:55.623415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_mod = ModifiedLlama.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "model_mod.eval().to(DEVICE).to(DTYPE)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")"
   ],
   "id": "fc76ac36ca500039",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:42:57.675210Z",
     "start_time": "2025-04-09T15:42:57.135899Z"
    }
   },
   "cell_type": "code",
   "source": "stepwise_perplexity(model_mod, tokenizer, texts, cache_impl=lambda: ModifiedCache())",
   "id": "90cc669d88c0f621",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Starting perplexity evaluation...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Backend doesn't support synchronizing streams.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mstepwise_perplexity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_mod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_impl\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mModifiedCache\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 23\u001B[39m, in \u001B[36mstepwise_perplexity\u001B[39m\u001B[34m(model, tokenizer, texts, cache_impl, update_every)\u001B[39m\n\u001B[32m     20\u001B[39m label_token = input_ids[i].unsqueeze(\u001B[32m0\u001B[39m)          \u001B[38;5;66;03m# [1]\u001B[39;00m\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m     output = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_slice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m     cache = output.past_key_values\n\u001B[32m     29\u001B[39m     logits = output.logits[:, -\u001B[32m1\u001B[39m, :]  \u001B[38;5;66;03m# [1, vocab_size]\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/kv-cache-reduce/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/kv-cache-reduce/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/kv-cache-reduce/.venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001B[39m, in \u001B[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[32m    169\u001B[39m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[32m    170\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/kv-cache-reduce/model/modeling_llama.py:893\u001B[39m, in \u001B[36mLlamaForCausalLM.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001B[39m\n\u001B[32m    890\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m    892\u001B[39m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m893\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    894\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    895\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    896\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    897\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    898\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    899\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    900\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    901\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    902\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    903\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    904\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    905\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    907\u001B[39m hidden_states = outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    908\u001B[39m \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/kv-cache-reduce/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/kv-cache-reduce/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/kv-cache-reduce/model/modeling_llama.py:632\u001B[39m, in \u001B[36mLlamaModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001B[39m\n\u001B[32m    620\u001B[39m     layer_outputs = \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(\n\u001B[32m    621\u001B[39m         decoder_layer.\u001B[34m__call__\u001B[39m,\n\u001B[32m    622\u001B[39m         hidden_states,\n\u001B[32m   (...)\u001B[39m\u001B[32m    629\u001B[39m         position_embeddings,\n\u001B[32m    630\u001B[39m     )\n\u001B[32m    631\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m632\u001B[39m     layer_outputs = \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    633\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    634\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    635\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    636\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    637\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    638\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    639\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    640\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    641\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mflash_attn_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    642\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    644\u001B[39m hidden_states = layer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    646\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/kv-cache-reduce/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/kv-cache-reduce/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/kv-cache-reduce/model/modeling_llama.py:374\u001B[39m, in \u001B[36mLlamaDecoderLayer.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[39m\n\u001B[32m    371\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.input_layernorm(hidden_states)\n\u001B[32m    373\u001B[39m \u001B[38;5;66;03m# Self Attention\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m374\u001B[39m hidden_states, self_attn_weights = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    375\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    376\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    377\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    378\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    379\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    380\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    381\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    382\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    383\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    384\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    385\u001B[39m hidden_states = residual + hidden_states\n\u001B[32m    387\u001B[39m \u001B[38;5;66;03m# Fully Connected\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/kv-cache-reduce/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/kv-cache-reduce/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/kv-cache-reduce/model/modeling_llama.py:311\u001B[39m, in \u001B[36mLlamaAttention.forward\u001B[39m\u001B[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001B[39m\n\u001B[32m    307\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(past_key_value, KVBiasCache) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(past_key_value, NaiveKVBiasCache)\n\u001B[32m    308\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(past_key_value, IterativeReduceKVBiasCache)):\n\u001B[32m    309\u001B[39m     \u001B[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001B[39;00m\n\u001B[32m    310\u001B[39m     cache_kwargs = {\u001B[33m\"\u001B[39m\u001B[33msin\u001B[39m\u001B[33m\"\u001B[39m: sin, \u001B[33m\"\u001B[39m\u001B[33mcos\u001B[39m\u001B[33m\"\u001B[39m: cos, \u001B[33m\"\u001B[39m\u001B[33mcache_position\u001B[39m\u001B[33m\"\u001B[39m: cache_position}\n\u001B[32m--> \u001B[39m\u001B[32m311\u001B[39m     key_states, value_states, kv_bias = \u001B[43mpast_key_value\u001B[49m\u001B[43m.\u001B[49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    312\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkey_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlayer_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_kwargs\u001B[49m\n\u001B[32m    313\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    314\u001B[39m     kwargs[\u001B[33m'\u001B[39m\u001B[33mkv_bias\u001B[39m\u001B[33m'\u001B[39m] = kv_bias\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    316\u001B[39m     \u001B[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/kv-cache-reduce/kvcache/iterative.py:197\u001B[39m, in \u001B[36mIterativeReduceKVBiasCache.update\u001B[39m\u001B[34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001B[39m\n\u001B[32m    194\u001B[39m     \u001B[38;5;28msuper\u001B[39m().update(key_states, value_states, layer_idx, cache_kwargs)\n\u001B[32m    195\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    196\u001B[39m     \u001B[38;5;66;03m# initiate async optimization for the next layer\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m197\u001B[39m     key_tensor, value_tensor, cluster_sizes = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mlayer_idx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m    199\u001B[39m     \u001B[38;5;66;03m# update cluster info\u001B[39;00m\n\u001B[32m    200\u001B[39m     \u001B[38;5;28mself\u001B[39m.cluster_sizes[layer_idx] = torch.cat([\n\u001B[32m    201\u001B[39m         cluster_sizes,\n\u001B[32m    202\u001B[39m         torch.ones(size=(batch_size, n_heads, seq_length), device=device, dtype=torch.long),\n\u001B[32m    203\u001B[39m     ], dim=-\u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/kv-cache-reduce/kvcache/iterative.py:153\u001B[39m, in \u001B[36mIterativeReduceKVBiasCache.__getitem__\u001B[39m\u001B[34m(self, layer_idx)\u001B[39m\n\u001B[32m    150\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Gets the cache for this layer. Optimizes the next layer.\"\"\"\u001B[39;00m\n\u001B[32m    152\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m layer_idx < \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m153\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptimize_stream\u001B[49m\u001B[43m.\u001B[49m\u001B[43msynchronize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    155\u001B[39m     key_tensor = \u001B[38;5;28mself\u001B[39m.key_cache[layer_idx]\n\u001B[32m    156\u001B[39m     value_tensor = \u001B[38;5;28mself\u001B[39m.value_cache[layer_idx]\n",
      "\u001B[31mRuntimeError\u001B[39m: Backend doesn't support synchronizing streams."
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1c4641849d167ebf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
